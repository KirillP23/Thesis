\graphicspath{ {ch4_figures/} }

We continue from the Section~\ref{intro:lift}, where we introduced the notion of graphlets and the problem statement of estimating graphlet counts using efficient Monte Carlo sampling scheme.

This Chapter is joint work with James Sharpnack, all results can be found on arXiv~\cite{Graphlets.2018}.

	\section{Prior sampling schemes}
	\label{sec:prior_algs}

\subsection{Subgraph Random Walk}
The subgraph random walk is described in \cite{Wang2014psrw}, where they make a modification called the pairwise subgraph random walk (PSRW).
Consider a set $\cV_k$ of all $k$-graphlets of $G$.
In order to perform a random walk where the states are $\cV_k$, we form the CIS-relationship graph.

Two $k$-CISs, $T,S \in \cV_k$ are connected with an edge if and only if vertex sets of $T$ and $S$ differ by one element, i.e. when $|V(T) \cap V(S)| = k-1$.
Given the graph structure, we sample $k$-CISs by a random walk on the set $\cV_k$, which is called Subgraph Random Walk (SRW).

Because the transition from state $S \in \cV_k$ is made uniformly at random to each adjacent CIS, then we know that the stationary distribution will sample each edge in the CIS-relationship graph with equal probability.
This fact enables \cite{Wang2014psrw} to provide a local estimator of the stationary probability $\pi(S)$.
PSRW is a modification of the SRW algorithm, where each transition is performed from $S$ to $T$ in $\cV_{k-1}$ and then the $k$-CIS $S \cup T$ is returned.
The graphlet count estimator is then calculated using Horovitz-Thompson inverse probability weighting~\ref{eq:moments}.

The mixing time is a critical issue for any MCMC, and subgraph sampling is no exception.
Dependence between consecutive samples results in a higher variability of $\hat N_m(G)$, and sufficient mixing is required for the stationary distribution to approximate the sampling distribution.
It was pointed out in~\cite{Bressan2017colourcoding} that the mixing time of the SRW can be of order $O(n^{k-2})$, even if the mixing time of the random walk on the original graph $G$ is of constant order $O(1)$.
PSRW also requires global constants based on the CIS-graph, which can be computationally intractable (super-linear time).

\subsection{Waddling Random Walk}

Another approach is to sample on ordered sequences of vertices $[v_1, \ldots, v_k]$, denoted by $A$, which would induce a $k$-CIS, $T = G|A$.
Given a sampling scheme of such sequences with probability $\tilde\pi(A)$, the estimator for graphlet counts is given by
\begin{equation}
  \label{eq:sequences}
  \hat N_m(G) := \frac{\omega_m}{n} \sum_{i=1}^n \frac{\ind(G|A_i\sim H_m)}{\tilde\pi(A_i)}
\end{equation}
for some fixed weights $\omega_m$.  
The main difference between these types of sampling is that we maintain the ordering of the vertices, while a CIS is an unordered set of vertices.

A naive method for sampling such sequences would be to perform a random walk on the graph $G$ and then sample the $k$ vertices most recently visited.
This scheme is appealing because it has an easy to compute stationary distribution, and can `inherit' the mixing rate from the random walk on $G$ (which is relatively small).
Despite these advantages, certain graphlet topologies, such as stars, will never be sampled, and modifications are needed to remedy this defect.
\cite{chen2016general} combined this basic idea with the SRW by maintaining a $l$ length history of the SRW on CISs of size $k-l+1$, and unioning the history, but this suffers from the same issues as SRW, such as slow mixing and the need to calculate global constants based on the CIS-graph.

\cite{Han2016waddling} introduced a {\em Waddling} protocol which retains a memory of the last $s$ vertices in the random walk on $G$ and then extends this subgraph by $k-s$ vertices from either the first or last vertex visited in the $s$-subgraph (this extension is known as the `waddle').
The authors provide a method for calculating the stationary distribution for this MCMC, and prove a bound on the error for the graphlet coefficients.
The downside to this method is that the precise Waddling protocol used should depend on the desired graphlet, and the algorithm involves a rejection step which may lead to a loss of efficiency.
In contrast, the lifting scheme has the advantage of inheriting the mixing time of the random walk on $G$, and it can simultaneously sample many CISs of differing sizes without any rejections.


\section{Subgraph lifting}
\label{sec:lifting}

	\subsection{Notation}
	For a subgraph $S \subseteq G$, denote $V_S$ to be the set of its vertices, $E_S$ to be the set of its edges.
	Denote $\cN_v(S)$ (vertex neighborhood of $S$) to be the set of all vertices adjacent to some vertex in $S$ not including $S$ itself. 
	Denote $\cN_e(S)$ (edge neighborhood of $S$) to be the set of all edges that connect a vertex from $S$ and a vertex outside of $S$.
	Also, denote $\deg(S)$ (degree of $S$) to be the number of edges in $\cN_e(S)$, and denote $\deg_S(u)$ ($S$-degree of $u$) to be the number of vertices from $S$ that are connected to $u$.
	Note that $\deg(S) + 2|E_S| = \sum_{v\in V_S} \deg(v)$.
	
	\subsection{Algorithm description}
	The lifting algorithm is based on a randomized protocol of attaching a vertex to a given CIS.
	For any \smash{$(k-1)$}-CIS, $S$ we lift it to a $k$-subgraph by adding a vertex from its neighborhood, $\cN_v(S)$ at random according to some probability distribution.
	Note that this basic lifting operation can explore any possible subgraph in $\cV_k$. 

	In this Chapter, we show that one can lift from a random walk on the vertices, or another vertex or edge sampling distribution, and achieve favorable properties.
	
\def\myliftgraphletwidth{0.10\textwidth}

\begin{figure}[th]
\hspace*{\fill}
\begin{subfigure}{\myliftgraphletwidth}
  \includegraphics[width=\linewidth]{lift_1.png}
  \caption{\label{fig:lift_1}}
\end{subfigure}\hfill
\begin{subfigure}{\myliftgraphletwidth}
  \includegraphics[width=\linewidth]{lift_2.png}
  \caption{\label{fig:lift_2}}
\end{subfigure}\hfill
\begin{subfigure}{\myliftgraphletwidth}
  \includegraphics[width=\linewidth]{lift_3.png}
  \caption{\label{fig:lift_3}}
\end{subfigure}\hfill
\begin{subfigure}{\myliftgraphletwidth}
  \includegraphics[width=\linewidth]{lift_4.png}
  \caption{\label{fig:lift_4}}
\end{subfigure}
\hspace*{\fill}
\vskip 5pt
\hspace*{\fill}
\begin{subfigure}{\myliftgraphletwidth}
  \includegraphics[width=\linewidth]{lift_5.png}
  \caption{\label{fig:lift_5}}
\end{subfigure}\hfill
\begin{subfigure}{\myliftgraphletwidth}
  \includegraphics[width=\linewidth]{lift_6.png}
  \caption{\label{fig:lift_6}}
\end{subfigure}\hfill
\begin{subfigure}{\myliftgraphletwidth}
  \includegraphics[width=\linewidth]{lift_7.png}
  \caption{\label{fig:lift_7}}
\end{subfigure}\hfill
\begin{subfigure}{\myliftgraphletwidth}
  \includegraphics[width=\linewidth]{lift_8.png}
  \caption{\label{fig:lift_8}}
\end{subfigure}\hspace*{\fill}
\caption{Lifting procedure}
\label{fig:lifting}
\end{figure}

    You can see an example of the lifting sampling scheme in Figure \ref{fig:lifting}, where the algorithm iteratively builds a $4$-CIS from a chosen node.
	%The lifting sampling scheme iteratively builds larger subgraphs from smaller ones, and you can see an example in Figure \ref{fig:lifting}.
	First assume we have a node $v_1$ sampled from the distribution $\pi_1$, a base distribution that can be computed from local information (fig.~\ref{fig:lift_1}). 
	We assume that $\pi_1(v) = \frac{f(\deg(v))}{K}$, where $f(x)$ is some function (usually a polynomial) and $K$ is some global normalizing constant which is assumed to be precomputed.
	Denote $S_1 = \{v_1\}$.
    To start our procedure, sample an edge $(v_1,v_2)$ uniformly from $\cN_e(S_1)$ (fig.~\ref{fig:lift_2}). 
	The vertex $v_2$ is then attached to $S_1$, forming a subgraph $S_2 = G|(V_{S_1} + {v_2})$ (fig.~\ref{fig:lift_3}).
	After that, we sample another edge $(v_i, v_3)$ (with $1\leq i\leq 2$) uniformly from $\cN_e(S_2)$, and the vertex $v_3$ is then attached to $S_2$ (fig.~\ref{fig:lift_4} - \ref{fig:lift_5}).
	At each step we sample an edge $(v_i, v_{r+1})$ (with $1\leq i \leq r$) from $\cN_e(S_r)$ uniformly at random, and attach the vertex $v_{r+1}$ to the subgraph $S_r$ (fig.~\ref{fig:lift_6} - \ref{fig:lift_8}).
	After $k-1$ operations, we obtain a $k$-CIS $T = S_k$.
	We'll refer to the procedure above as the {\em lifting procedure starting at vertex $v_1$}.

    By induction, we can see that every $k$-CIS has a non-zero probability of being visited, assuming that $\pi_1$ is supported on every vertex.
	The Waddling method was motivated by the fact that a simple random walk would not visit all subgraphs in $\cV_k$, yet Waddling only partially solved this issue, because not every wadding protocol can sample every $k$-graphlet.
    Typically, $\pi_1$ is assumed to be the stationary distribution of a simple random walk, but it could be another distribution such as uniform sampling over the vertices or edges.
	One motivation for lifting is that if we sample the vertices from a simple random walk, then lifting `inherits' its mixing time, much like Waddling.
	Hence, lifting is a simple algorithm that can sample any CIS with good mixing rates and no rejections.
	It remains to be demonstrated that we can calculate the probability of sampling the $k$-CIS $\pi(S)$ using only local information.
	
	\subsection{Ordered lift estimator}
	
	%When we sample a $k$-CIS, we match this to a $k$-graphlet through a graph isomorphism algorithm.
	%Alternatively, we can maintain an ordering of the vertices, and count {\em ordered $k$-CISs}, which is are $k$-CISs where the vertices are labelled from $1,\ldots,k$.
 %   Then at the end of the algorithm we combine the ordered $k$-graphlet counts that correspond to the same topology.
    
    We can think of a lifting procedure as a way of sampling a sequence $A = [v_1, \ldots, v_k]$, ordered from the first vertex sampled to the last, that is then used to generate a CIS.
	Denote the set of such sequences as $V^k_G$.
%	For $A = [v_1, \ldots, v_k] \in V^k_G$, denote $A[i]:=v_i$.
	Let $S_r = G|\{v_1,\ldots, v_r\}$ be the $r$-CIS obtained by the lifting procedure on step $r$.
	The probability of sampling vertex $v_{r+1}$ on the step $r+1$ is equal to
	\begin{equation*}
	    \mathbb{P}(v_{r+1}|S_{r}) := \frac{\deg_{S_r}(v_{r+1})}{|\cN_e(S_r)|} =
	    \frac{|E_{S_{r+1}}| - |E_{S_r}|}{\sum_{i=1}^r \deg(v_i) - 2|E_{S_r}|}.
	\end{equation*}
	Thus, the probability of sampling a sequence $A \in V^k_G$ is equal to
	\begin{equation}
	\label{eq:tildepi}
	    \Tilde\pi(A) := \pi_1(v_1)\prod_{r=1}^{k-1} \P\left(v_{r+1}|S_r\right) =
	    \frac{f(\deg(v_1))}{K} \prod_{r=1}^{k-1} \frac{|E_{S_{r+1}}| - |E_{S_r}|}{\sum_{i=1}^r \deg(v_i) - 2|E_{S_r}|}.
	\end{equation}
    Critically, this equation can be computed with only neighborhood information about the involved vertices, so it takes $O(k)$ neighborhood queries.
    Because there are many orderings that could have led to the same CIS $T$, then we need to apply proper weights in the graphlet count estimate \eqref{eq:sequences} by enumerating the number of possible orderings.
    
	Consider the sampled $k$-CIS $T := S_k$.
	Denote the set of possible sequences $A = [v_1,\ldots, v_k]$ that would form $T$ in the lifting process as $\co(T)$.
	Notice that $S_r = G|\{v_1,\ldots,v_r\}$ must be a connected subgraph for all $r$.
	Thus,
	\begin{equation}
	\label{eq:co}
	    \co(T) = \big\{\,[v_1,\ldots,v_k]\in V^k_G \mid \{v_1,\ldots,v_k\} = V_T,  T|\{v_1,\ldots,v_r\} \text{ is connected }\big\}.
	\end{equation}
	Since the elements of $\co(T)$ are just certain orderings of vertices in $T$, we call an element from $\co(T)$ a \textit{compatible ordering} of $T$.
	Note that $|\co(T)|$ only depends on the type of the graphlet isomorphic to $T$, and it can be precomputed using dynamic programming.
	Thus, when $T\sim H_m$, the number of compatible orderings are equal: $|\co(H_m)| = |\co(T)|$. Note that $|\co(H_m)|$ can vary from $2^{k-1}$ (for $k$-path) to $k!$ (for $k$-clique).
	We set up the estimator from \eqref{eq:sequences} as
	\begin{equation}
	\label{eq:ord_est}
	    \hat N_{O,m} := \frac{1}{n}\frac{1}{|\co(H_m)|} \sum_{i=1}^n \frac{\ind(G|A_i\sim H_m)}{\tilde\pi(A_i)}.
	\end{equation}
	We call it the {\em ordered lift estimator} for the graphlet count.

\begin{algorithm}[h]
\label{alg:OLE}
\caption{Ordered Lift Estimator (with optional shotgun sampling)}
\begin{algorithmic}
    \INPUT Graph $G$, $k$-graphlet $H_m$
    \OUTPUT $\hat N_m(G)$
    \STATE Count $|\co(H_m)|$- the number of compatible orderings in $H_m$.
    \STATE Initialize $v$ at an arbitrary node, $n \gets 0$, $\hat N_m(G) \gets 0$
    \WHILE{stopping criteria is not met}
        \STATE Sample $v_1$ from $\pi_1(v)$
        \STATE Initialize $V_S \gets \{v_1\}$ and $E_S \gets \{\}$
        \STATE Initialize $\cN_e(S)\gets \cN_e(v_1)$
        \STATE Initialize $\pi(S) \gets \pi_1(v_1)$
        \WHILE{$|V_S| < k-1$}
            \STATE Sample an edge $e=(v,u)$ uniformly from $\cN_e(S)$, with $v\in V_S$ and $u\notin V_S$
            \STATE Set $E_S(u) \gets \{(v,u)\in \cN_e(S)\}$
            \STATE Update $\pi(S)\gets \pi(S)\frac{|E_S(u)|}{|\cN_e(S)|}$
            \STATE Update $V_S\gets V_S\cup\{u\}$ and $E_S \gets E_S \cup E_S(u)$
            \STATE Query $\cN_e(u)$
            \STATE Update $\cN_e(S) \gets [\cN_e(S)\cup\cN_e(u)]\setminus E_S(u)$
        \ENDWHILE
        \IF {not shotgun sampling}
            \STATE Sample an edge $e=(v,u)$ uniformly from $\cN_e(S)$, with $v\in V_S$ and $u\notin V_S$
            \STATE Set $E_S(u) \gets \{(v,u)\in \cN_e(S)\}$
            \STATE Set $\pi(T)\gets \pi(S)\frac{|E_S(u)|}{|\cN_e(S)|}$
            \STATE Set $V_T\gets V_S\cup\{u\}$ and $E_T \gets E_S\cup E_S(u)$
            \IF {$(V_T, E_T) \sim H_m$}
                \STATE Update $\hat N_m(G) \gets \hat N_m(G) + \pi^{-1}(T)$ 
            \ENDIF
        \ENDIF
        \IF {shotgun sampling}
            \FORALL{$u\in \cN_v(S)$}
                \STATE Set $E_S(u) \gets \{(v,u)\in \cN_e(S)\}$
                \STATE Set $V_T\gets V_S\cup\{u\}$ and $E_T \gets E_S \cup E_S(u)$
                \IF{$(V_T, E_T) \sim H_m$}
                    \STATE Update $\hat N_m(G) \gets \hat N_m(G) + \pi^{-1}(S)$
                \ENDIF
            \ENDFOR
        \ENDIF
        \STATE Update $n \gets n + 1$
    \ENDWHILE
    \STATE Normalize $\hat N_m(G) \gets \frac{1}{n}\frac{1}{|\co(H_m)|}\hat N_m(G)$
\end{algorithmic}
\end{algorithm}
	
	A drawback of the algorithm is that it takes $k-1$ queries to lift the CIS plus the number of steps required to sample the first vertex (when sampled from Markov chain). 
	To increase the number of samples per query, notice that if we sample $B = [v_1, \ldots, v_{k-1}]$ via lifting, we can get subgraphs induced by $A = [v_1,\ldots, v_{k-1},u]$ for all $u \in \cN_v(B)$ without any additional queries.
	
	Thus, for each sampled sequence $B_i \in V_G^{k-1}$, we can compute the sum $\sum_{u\in \cN_v(B_i)} \ind(G|B_i \cup \{u\} \sim H_m)$ to incorporate the information about all $k$-CISs in the neighborhood of $B_i$.
	We call this procedure {\em shotgun sampling}.
	The corresponding estimator based on \eqref{eq:sequences} is
	\begin{equation}
	\label{eq:shot_est}
	    \hat N_{S,m} = \frac{1}{n}\frac{1}{|\co(H_m)|} \sum_{i=1}^n \frac{\sum_{u \in \cN_v(B_i)} \ind(G|B_i \cup \{u\} \sim H_m)}{\tilde \pi(B_i)}.
	\end{equation}
	Shotgun sampling produces more CIS samples with no additional query cost, but the CIS samples generated in a single iteration will be highly dependent.
	%independence of CIS samples from two consecutive shotgun shots heavily relies on the well-mixing of sampled sequences $B_i$.
	The following proposition states that the resulting estimators are unbiased.
	
	\begin{proposition}
	\label{prop:unbiased}
    The ordered lifted estimator, $\hat N_{O,m}$, and the shotgun estimator, $\hat N_{S,m}$, are unbiased for the graphlet counts $N_m$.
    \end{proposition}
	
% 	Shotgun sampling is useful in cases when the number of queries is limited, but has a drawback of high correlation between samples.
% 	If the initial vertex $v_1$ was sampled from a random walk, the number of steps between initial vertices should be chosen so that shotgun shots are sufficiently far apart from each other.
	
	\subsection{Unordered lift estimator}
	
	The ordered lift estimators computed the probability of sampling the CIS and the order of included vertices.
	Alternatively, we can compute the marginal probability of sampling the unordered graphlet, $\pi_U(T)$ for the lifted CIS $T \in \cV_k(G)$.
    One advantage of this approach is that this probability is a function of only the degrees of vertices $V_T$.
	%We can also use the estimator \eqref{eq:moments} for the sampling space of $k$-graphlets $\cV_k(G)$.
	%In order to do that, we need to compute the probability $\pi_k(T)$ of getting a particular $k$-graphlet $T \in \cV_k(G)$ in the lifting procedure.
	This can be done either recursively or directly. 
	Throughout, let the set of vertices of $T$ be $v_1,\ldots,v_k$.
	
	We begin the algorithm by querying the probability of obtaining any vertex in $T$, $\pi_1(v_i), i=1,\ldots,k$.
	%As with the Recursive formula starts with known probabilities of initial vertices $\pi_1(v_i)$.
	We will build the probability of obtaining any connected subgraph of $T$ inductively.
	This is possible because the probability of getting $T$ via lifting is given by the sum $\pi_U(T) = \sum_S \mathbb{P}(T|S) \pi_{U}(S)$, where the sum is taken over all connected \smash{$(k-1)$}-subgraphs $S\subset T$, and $\mathbb{P}(T|S)$ denotes the probability of getting from $S$ to $T$ in the lifting procedure.
	%Let $T\setminus S$ be the unique vertex of $T$ that is not in $S$.
	Then
	\begin{equation}
	\label{eq:recursive}
		\pi_{U}(T) = \sum_{S\subset T} \pi_{U}(S) \frac{\deg_S (V_T\setminus V_S)}{|\cN_e(S)|} = 
		\sum_{S\subset T} \pi_{U}(S) \frac{|E_T| - |E_S|}
		{\sum_{u\in S} \deg(u) - 2|E_S|},
	\end{equation}
	where the sum is taken over all connected \smash{$(k-1)$}-subgraphs $S\subset T$.
	
	For a direct formula, we notice that $\pi_U(T) = \sum_{A\in \co(T)} \tilde\pi(A)$, where $\co(T)$ is the set of compatible orderings of $T$ from previous section, and $\tilde\pi(A)$ is the probability of getting sequence $A\in \co(T)$ in the lifting process (see \eqref{eq:co},\eqref{eq:tildepi}).
	Then 
	\begin{equation}
	\label{eq:direct}
		\pi_U(T) = \sum_{A\in\co(T)} \frac{f(\deg(A[1]))}{K} \prod_{r=1}^{k-1} \frac{|E_{S_{r+1}(A)}| - |E_{S_r(A)}|}{\sum_{i=1}^r \deg(A[i]) - 2|E_{S_r(A)}|},
	\end{equation}
	where, given $A=[v_1,\ldots,v_k]$, $A[i]$ is the $i$th vertex in $A$ and $S_r(A) = G|\{v_1,\ldots,v_r\}$.
	
	Although calculation of this probability on-the-fly is cost-prohibitive, we can greatly reduce the number of operations by noticing that the probability $\pi_k(T)$ is a 
	function of degrees of the vertices: for a CIS $T$ of type $m$, let $[v_1, \ldots, v_k]$ be an arbitrary labelling of the vertices of $T$ with $d_i = \deg(v_i)$, then the probability of $T$ is
	\begin{equation*}
	    \pi_U(T) =\frac{1}{K} F_m(d_1, \ldots, d_k)
	\end{equation*}
    for a cached function $F_m$ given by \eqref{eq:direct}.
	
	\textbf{Example.} Consider a triangle, which is a 3-graphlet with edges $(v_1,v_2)$, 
	$(v_2, v_3)$ and $(v_1,v_3)$. Given the degrees $d_1, d_2, d_3$ of the corresponding 
	vertices, the probability function is
	\begin{multline}
	\label{prob:triangle}
		\pi_U(\mathrm{triangle}) = \left( \frac{\pi_1(d_1)}{d_1} + 
		\frac{\pi_1(d_2)}{d_2}\right) \frac{2}{d_1+d_2-2} +\\ 
		+ \left( \frac{\pi_1(d_2)}{d_2} + \frac{\pi_1(d_3)}{d_3}\right) 
		\frac{2}{d_2+d_3-2} +
		\left( \frac{\pi_1(d_3)}{d_3} + \frac{\pi_1(d_1)}{d_1}\right) 
		\frac{2}{d_3+d_1-2}.
	\end{multline}
	
	\textbf{Example.} Consider a wedge, which is a 3-graphlet with edges $(v_1,v_2)$ and 
	$(v_1,v_3)$. Given the degrees $d_1, d_2, d_3$ of the corresponding 
	vertices, the probability function is
	\begin{equation}
	\label{prob:wedge}
		\pi_U(\mathrm{wedge}) = \left( \frac{\pi_1(d_1)}{d_1} + \frac{\pi_1(d_2)}{d_2}\right) \frac{1}{d_1+d_2-2} +\\
		\left( \frac{\pi_1(d_1)}{d_1} + \frac{\pi_1(d_3)}{d_3}\right) \frac{1}{d_1+d_3-2}.
	\end{equation}

	We need to only compute functions $F_m$ once before starting the algorithm.
	When a $k$-CIS $T$ is sampled via lifting procedure, we find the natural labelling of vertices in $T$ via the isomorphism $H_m \rightarrow T$, and use the function $F_m$ together with the degrees $d_1,\ldots,d_k$ of vertices of $T$ to compute the value of 
	$\pi_U(T) = \frac{1}{K} F_{m}(d_1,\ldots,d_k)$.
	
%	Recursive formula \eqref{eq:recursive} is useful for analysis of graphlet counts $N_m(G)$ for all types $m$, and direct formula \eqref{eq:direct} is useful for analysis of a specific graphlet type $m$.
	
\begin{algorithm}[h]
\label{alg:ULE}
\caption{Unordered Lift Estimator}
\begin{algorithmic}
    \INPUT Graph $G$, $k$-graphlet $H_m$
    \OUTPUT $\hat N_m(G)$
    \STATE Set an ordering $[1,\ldots, k]$ on the vertices of $H_m$, precompute the function $F_m(d_1,\ldots, d_k)$ and the global constant $K$
    \STATE Initialize $v$ at an arbitrary node, $n \gets 0$, $\hat N_m(G) \gets 0$
    \WHILE{stopping criteria is not met}
        \STATE Sample initial vertex $v$ from $\pi_1(v)$
        \STATE Initialize $V_T \gets \{v\}$ and $E_T \gets \{\}$
        \STATE Initialize $\cN_e(T)\gets \cN_e(v)$
        \WHILE{$|V_T| < k$}
            \STATE Sample an edge $e=(v,u)$ uniformly from $\cN_e(T)$, with $v\in V_T$ and $u\notin V_T$
            \STATE Set $E_T(u) \gets \{(v,u)\in \cN_e(T)\}$
            \STATE Update $V_T\gets V_T\cup\{u\}$ and $E_T \gets E_T \cup E_T(u)$
            \STATE Query $\cN_e(u)$
            \STATE Update $\cN_e(T) \gets [\cN_e(T)\cup\cN_e(u)]\setminus E_T(u)$
        \ENDWHILE
        \IF{$(V_T, E_T)\sim H_m$}
            \STATE Determine the ordering $[v_1,\ldots,v_k]$ of vertices in $V_T$ induced by the isomorphism $(V_T, E_T)\sim H_m$
            \STATE Set $d_i = |\cN_e(v_i)|$ for all $i=1,\ldots, k$
            \STATE Set $\pi(T) = \frac{1}{K}F_m(d_1,\ldots,d_k)$
            \STATE Update $\hat N_m(G) \gets \hat N_m(G) + \pi^{-1}(T)$
        \ENDIF
       \STATE Update $n \gets n + 1$
    \ENDWHILE
    \STATE Normalize $\hat N_m(G) \gets \frac{1}{n}\hat N_m(G)$
\end{algorithmic}
\end{algorithm}

	\subsection{Sampling a starting vertex}
	
	One advantage of the lifting protocol is that it can be decoupled from the selection of a starting vertex, and our calculations remained agnostic to the distribution $\pi_1$ (although, we did require that it was a function of the degrees).
    There are two method that we would like to consider, one is the uniform selection over the set of vertices, and the other is from a random walk on the vertices, that presumably has reached its stationary distribution. 
%     \noindent
% 	\textbf{Uniform vertex sampling.} 
	
	Consider sampling the starting vertex $v$ independently and from an arbitrary distribution $\pi_1$ when we have access to all the vertices.
    The advantage of sampling vertices independently, is that the lifting process will result in independent CIS samples.
    A byproduct of this is that the variance of the graphlet count estimator \eqref{eq:moments} can be decomposed into the variance of the individual CIS samples.
    Given iid draws, the variance of the estimator $\hat N_m(G)$ is then  
	\begin{equation}
	\label{eq:variation.ind}
	    V_{m}^{\independent}(\hat N_{U,m}) := \frac 1n \Var\left( \frac{\ind(T_n \sim H_m)}{\pi_U(T_n)} \right)
	    %\frac{1}{n}\Var_{\pi_U}\left(\frac{\ind(T\sim H_m)}{\pi_U(T)}\right) =\\
	    =\frac{1}{n}\left(\sum_{T \in \cV_k} \frac{\ind(T\sim H_m)}{\pi_U(T)} - N_m(G)^2\right),
	\end{equation}
	which is small when the distribution of  $\pi_U(T)$ is close to uniform distribution on $\cV_m(G)$.
	Equation \eqref{eq:variation.ind} demonstrates fundamental property that when $\pi_U(T)$ is small then it contributes more to the variance of the estimator.
	The variation in \eqref{eq:variation.ind} can be reduced by an appropriate choice of $\pi_1$, i.e.~the starting distribution.
	
	For example, if $k=3$, let $\pi_1(v) = \frac{1}{K}\deg(v)(\deg(v)-1)$, where 
	$K=\sum_{u\in V_G} \deg(u)(\deg(u)-1)$. 
	Then by \eqref{prob:triangle} and \eqref{prob:wedge}
	\begin{equation*}
		\pi_U(\mathrm{triangle}) = \frac{6}{K}, \quad \pi_U(\mathrm{wedge}) = \frac{2}{K}.
	\end{equation*}
	Calculating $K$ takes $O(|V_G|)$ operations (preparation), sampling starting vertex $v$ takes $O(\log(|V_G|))$ operations, and lifting takes $O(\Delta)$, where $\Delta$ is the maximum vertex degree in $G$.

% 	\vskip 10pt
% 	\noindent
% 	\textbf{Random walk vertex sampling.} 
	When we don't have access to the whole graph structure, a natural choice is to run a simple random walk (with transitional probabilities \smash{$p(i \small \to j) = \frac{1}{\deg(i)}$} whenever $j$ in connected to $i$ with an edge).
	Then the stationary distribution is $\pi_1(v) = \deg(v) / (2|E_G|),$ and we can calculate all probabilities $\pi_k$ accordingly.
	One feature of the simple random walk is that the resulting edge distribution is uniform: $\pi_U(e) = \frac{1}{|E_G|}$ for all $e\in E_G$ (edges are $2$-graphlets). 
	Therefore, the probabilities $\pi_U$ are the same as if sampling an edge uniformly at random and start lifting procedure from that edge.
	

\section{Theoretical Analysis of Lifting}

As long as the base vertex distribution, $\pi_1$, is accurate then we have that the graphlet counts are unbiased for each of the aforementioned methods.
The variance of the graphlet counts will differ between these methods and other competing algorithms such as Waddling and PSRW.
The variance of sampling algorithms can be decomposed into two parts, an independent sample variance component and a between sample covariance component.
As we have seen the independent variance component is based on the properties of $\pi$ resulting from the procedure (see \eqref{eq:variation.ind}).
The covariance component will be low if the samples are not highly dependent on the past, namely that the Markov chain is well mixed.
%Throughout this section, let $\pi$ indicate any of $\pi_U, \pi_O, \pi_S$ depending on the lifting procedure.
We have three different estimators: Ordered Lift estimator $\hat N_{O,m}$, Shotgun Lift estimator $\hat N_{S,m}$ and Unordered Lift estimator $\hat N_{U,m}$.
For each estimator, we sample different objects: sequences $A_i \in V_G^k$ for Ordered, sequences $B_i \in V_G^{k-1}$ for Shotgun, and CISs $T_i\in \cV_k(G)$ for Unordered estimator.
Throughout this section, we will denote 
\begin{enumerate}
    \item for the Ordered Lift estimator,
    \begin{equation}
    \label{phi:ord}
        \phi_{O,i} = \frac{\ind(G|A_i\sim H_m)}{|\co(H_m)|\tilde\pi(A_i)},
    \end{equation} 
    
    \item for the Shotgun Lift estimator,
    \begin{equation}
    \label{phi:shot}
        \phi_{S,i} =  \frac{\sum_{u \in \cN_v(B_i)} \ind(G|B_i \cup \{u\} \sim H_m)}{|\co(H_m)| \tilde \pi(B_i)},
    \end{equation}
    
    \item for the Unordered Lift estimator,
    \begin{equation}
    \label{phi:unord}
        \phi_{U,i} = \frac{\ind(T_i\sim H_m)}{\pi_U(T_i)}.
    \end{equation}
    
\end{enumerate}

Note that $N_m(G) = \E \phi_1$, and $\hat N_m(G) = \frac{1}{n}\sum_i \phi_i$ for the corresponding estimators.


%\subsection{Bounding variance in $\hat N_m$}

The variance can be decomposed into the independent sample variance and a covariance term,
\begin{equation}
\label{eq:variance}
\Var(\hat N_m(G)) = \frac{1}{n}V_m^{\independent}(\phi_1) + \frac{2}{n^2} \sum_{i < j} \mathbb \Cov \left( \phi_i, \phi_j \right).
\end{equation}
For Markov chains, the summand in the second term will typically decrease exponentially as the lag $j-i$ increases, due to mixing.
Let us focus on the first term, with the goal of controlling this for either choice of base vertex distribution, $\pi_1$, and the lifting scheme.

% We hypothesized that the lifted MCMC will have faster mixing times than the graphlet MCMC.
% The lifting is performed after a sample from the underlying graph MCMC, so in order for the lifting procedure to 


\begin{theorem}
  \label{thm:var_bd}
  Let $\phi_1$ be as defined in \eqref{phi:ord}, \eqref{phi:shot} or \eqref{phi:unord}.
  Denote the first $k$ highest degrees of vertices in $G$ as $\Delta_1,\ldots, \Delta_k$ and denote $D = \prod_{r=2}^{k-1} (\Delta_1 +\ldots + \Delta_r)$.
  
  (1) If $\pi_1$ is the stationary distribution of the vertex random walk then
    \begin{equation}
    V_m^{\independent}(\phi_1) \leq N_m(G) \frac{2|E_G|}{|\co(H_m)|} D.
\end{equation}
  (2) If $\pi_1$ is the uniform distribution over the vertices then 
  \begin{equation}
      V_m^{\independent}(\phi_1) \leq N_m(G) \frac{2 \Delta_1 |E_G|}{|\co(H_m)|} D.
  \end{equation}
\end{theorem}
This result is comparable to analogous theorems for Waddling, \cite{Han2016waddling}, and PSRW, \cite{Wang2014psrw}.

%\subsection{Mixing time of lifted MCMC}

When the vertices are sampled independently, the covariance term disappears, so we will focus on the sampling vertices via random walk in this subsection.
One advantage of the lifting procedure over the SRW is that it inherits the mixing properties from the vertex random walk.
This can be thought of as a consequence of the data processing inequality in that the lifted CISs are no more dependent then the starting vertices from which they were lifted.
To that end, let us review some basics about mixing of Markov chains,

% 	More importantly, in order to get two $\varepsilon$-uncorrelated samples $T_i$ and $T_{i+1}$, we only need to sample two $\varepsilon$-uncorrelated starting vertices.

% 	\begin{definition}
% 		Given a random walk with transitional probabilities $\P(i\rightarrow j)$ and 
% 		stationary distribution $\pi(i)$ on graph $G$, the mixing time is defined as
% 		\begin{equation*}
% 			\tau_G(\varepsilon) = 
% 			\min\left\{t \mid 
% 			\sum_{j\in G} \left\vert \P^{(t)}(i\rightarrow j) - \pi(j)\right\vert
% 			<\varepsilon \quad \forall i\in G\right\}.
% 		\end{equation*}
% 	\end{definition}
	
% 	Mixing time of the simple walk on $G$ can be bounded by the following formula from \cite{Sinclair1992}:
% 	\begin{equation*}
% 		\frac{\mu}{2(1-\mu) \log(\frac{1}{2\varepsilon})} \leq
% 		\tau_G(\varepsilon) \leq
% 		\frac{\log(n) + \log(\frac{1}{\varepsilon})}{1-\mu},
% 	\end{equation*}
% 	where $\mu$ is the second largest (by absolute value) eigenvalue of the adjacency matrix 
% 	of $G$, and $n=|V_G|$
	
% 	Taking $\kappa  = (1-\mu)^{-1}$, we'll use the mixing time $\tau_G(\varepsilon) \approx \kappa(\log(n)+\log(\varepsilon^{-1}))$ as a number of steps required to sample a starting vertex.
% 	In turn, that will generate two $\varepsilon$-uncorrelated graphlet samples (see section ??).
% 	Note that $\kappa$ can vary from 20 to 500 for different social graphs (for more information about mixing time in social graphs, see \cite{Mohaisen2010mixingtime}).
	
% 	On the other hand, it has been shown in \cite{Bressan2017colourcoding} that the mixing time of SRW can be as large as $\Omega(n^{k-2})$, even when the graph $G$ has mixing time of constant order.
% 	The lifting procedure allows us to mix samples on the level of vertices instead of the level of subgraphs, making the mixing much faster.

% 	There is a choice of mixing steps $\tau_{mix}$ that we need to make.
% 	This choice would depend on the cost of queries: if queries are cheap, we'll do 
% 	$\tau_{mix} \approx \kappa\log(n)$, and if the queries are expensive or limited, 
% 	we choose some constant $\tau_{mix}$.
% 	Further discussion on the choice of $\tau_{mix}$ when the number of queries is limited 
% 	will be based on empirical results.


\begin{definition}
  Define the mixing coefficient of a stationary Markov chain with discrete state space $X_t \in \mathcal X$ as
  \begin{equation}
  \label{eq:gamma}
      \gamma_X(h) = \frac{1}{2}\max_{x_1\in \mathcal X} \sum_{x_2 \in \mathcal X} |\P(X_{t+h} = x_2, X_t = x_1) - \pi(x_1) \pi(x_2)|,
  \end{equation}
  where $\pi(x)$ is the stationary distribution of the Markov chain.
  Also, define the mixing time of a stationary Markov chain $\{X_t\}$ as
  \begin{equation}
      \tau_X(\varepsilon) = \min\left\{\, h \mid \gamma_X(h) < \varepsilon  \right\}.
  \end{equation}
\end{definition}

\begin{theorem}\cite{Sinclair1992}
  Given stationary Markov chain $\{X_t\}$ with $\mu < 1$ being the second largest eigenvalue of the transitional matrix,
  \begin{equation}
      \gamma_X(h) \leq e^{-(1-\mu)h}.
  \end{equation}
\end{theorem}

There are two consequences of mixing for CIS sampling.
First, an initial burn-in period is needed for the distribution $\pi$ to converge to the stationary distribution (and for the graphlet counts to be unbiased).
Second, by spacing out the samples with intermediate burn-in periods and only obtaining CISs every $h$ steps we can reduce the covariance component of the variance of $\hat N_m$.
Critically, if we wish to wait for $h$ steps, we do not need to perform the lifting scheme in the intervening iterations, since those graphlets will not be counted.
So, unlike in other MCMC method, spacing in lifted CIS sampling is computationally very inexpensive.
Because burn-in is a one-time cost and requires only a random walk on the graph, we will suppose that we begin sampling from the stationary distribution, and the remaining source of variation is due to insufficient spacing between samples.
The following theorem illustrates the point that the lifted MCMC inherits mixing properties from the vertex random walk.

\begin{theorem}
\label{thm:cor_bd}
Consider sampling a starting vertex from a random walk, such that a sufficient burn in period has elapsed and stationarity has been reached.
Let $h$ be the spacing between the CIS samples, $D$ be defined as in Theorem \ref{thm:var_bd}, and $\mu$ be the second largest eigenvalue of the transition matrix for the vertex random walk.
Let $\phi_i$ be as defined in \eqref{phi:ord}, \eqref{phi:shot} or \eqref{phi:unord}, then
\begin{equation*}
    \left\vert \Cov\left(\phi_i, \phi_{i+1})\right)\right\vert \leq
    8 N_m(G)|E_G|^2 e^{- (1-\mu) h} D.
\end{equation*}
\end{theorem}

\begin{corollary}
In the notation of the Theorem~\ref{thm:cor_bd},
\begin{equation*}
    \frac{2}{n} \left\vert\sum_{i < j} \Cov\left(\phi_i, \phi_j\right) \right\vert \leq
     8 N_m(G)|E_G|^2 \frac{e^{- (1-\mu) h}}{1- e^{- (1-\mu) h}} D.
\end{equation*}
\end{corollary}

Hence, if we allow $h$ to grow large enough then we can reduce the effect of the covariance term, and our CISs will seem as if they are independent samples.

\section{Experiments}

For our experiments, we picked five networks of different size, density and domain.
All networks are available online in the network repository~\cite{nr-aaai15}.
The corresponding graphs are undirected, and were preprocessed to be simple and unweighted.

\begin{center}
        \begin{tabular}{ | l || r | r | }
        \hline
        \multicolumn{3}{|c|}{Network information} \\
        \hline
        Network name & $|V_G|$ & $|E_G|$ \\ \hline
        \textbf{bio-celegansneural} & 297 & 2148 \\
        \textbf{ia-email-univ} & 1133 & 5451 \\
        \textbf{misc-polblogs} & 1224 & 16718 \\
        \textbf{misc-as-caida} & 26475 & 52281 \\
        \textbf{misc-fullb} & 199187 & 5754445 \\
        \hline
    \end{tabular}
\end{center}

\def\myplotwidth{0.23\textwidth}

\begin{figure}[th!]
\hspace*{\fill}
\minipage{\myplotwidth}
  \includegraphics[width=\linewidth]{bio-celegansneural_errors_H_1.pdf}
\endminipage\hfill
\minipage{\myplotwidth}
  \includegraphics[width=\linewidth]{bio-celegansneural_errors_H_2.pdf}
\endminipage\hfill
\minipage{\myplotwidth}
  \includegraphics[width=\linewidth]{ia-email-univ_errors_H_1.pdf}
\endminipage\hfill
\minipage{\myplotwidth}
  \includegraphics[width=\linewidth]{ia-email-univ_errors_H_2.pdf}
\endminipage
\hspace*{\fill}
\vskip 5pt
\hspace*{\fill}
\minipage{\myplotwidth}%
  \includegraphics[width=\linewidth]{misc-polblogs_errors_H_1.pdf}
\endminipage\hfill
\minipage{\myplotwidth}
  \includegraphics[width=\linewidth]{misc-polblogs_errors_H_2.pdf}
\endminipage\hfill
\minipage{\myplotwidth}%
  \includegraphics[width=\linewidth]{misc-as-caida_errors_H_1.pdf}
\endminipage\hfill
\minipage{\myplotwidth}
  \includegraphics[width=\linewidth]{misc-as-caida_errors_H_2.pdf}
\endminipage
\hspace*{\fill}
\vskip 5pt
\hspace*{\fill}
\minipage{\myplotwidth}%
  \includegraphics[width=\linewidth]{misc-fullb_errors_H_1.pdf}
\endminipage\hfill
\minipage{\myplotwidth}
  \includegraphics[width=\linewidth]{misc-fullb_errors_H_2.pdf}
\endminipage
\hspace*{\fill}
\caption{Relative errors of count estimators for $H_1^{(3)}$ and $H_2^{(3)}$.}
\label{fig:relative_errors}
\end{figure}

\begin{figure}[th!]
\hspace*{\fill}
\minipage{\myplotwidth}
  \includegraphics[width=\linewidth]{bio-celegansneural_H_1.pdf}
\endminipage\hfill
\minipage{\myplotwidth}
  \includegraphics[width=\linewidth]{bio-celegansneural_H_2.pdf}
\endminipage\hfill
\minipage{\myplotwidth}%
  \includegraphics[width=\linewidth]{ia-email-univ_H_1.pdf}
\endminipage\hfill
\minipage{\myplotwidth}
  \includegraphics[width=\linewidth]{ia-email-univ_H_2.pdf}
\endminipage
\hspace*{\fill}
\vskip 5pt
\hspace*{\fill}
\minipage{\myplotwidth}
  \includegraphics[width=\linewidth]{misc-polblogs_H_1.pdf}
\endminipage\hfill
\minipage{\myplotwidth}
  \includegraphics[width=\linewidth]{misc-polblogs_H_2.pdf}
\endminipage\hfill
\minipage{\myplotwidth}%
  \includegraphics[width=\linewidth]{misc-as-caida_H_1.pdf}
\endminipage\hfill
\minipage{\myplotwidth}
  \includegraphics[width=\linewidth]{misc-as-caida_H_2.pdf}
\endminipage
\hspace*{\fill}
\vskip 5pt
\hspace*{\fill}
\minipage{\myplotwidth}%
  \includegraphics[width=\linewidth]{misc-fullb_H_1.pdf}
\endminipage\hfill
\minipage{\myplotwidth}
  \includegraphics[width=\linewidth]{misc-fullb_H_2.pdf}
\endminipage
\hspace*{\fill}
\caption{Correlation of $\phi_i$ and $\phi_{i+1}$ depending on the intermediate burn-in time, $h$, between samples for graphlets $H_1^{(4)}$ and $H_2^{(4)}$.}
\label{fig:correlation}
\end{figure}

%\subsection{Implementation details}

We demonstrate performance of the shotgun and unordered lift method compared to two competitive methods: PSRW and Waddling.
All algorithms were implemented in Python, and the code is available on GitHub\footnote{\url{github.com/KirillP23/LiftSRW}}.

We will compare the effectiveness of estimators in two ways:
\begin{enumerate}
\item Relative error of a graphlet count estimator with $k=3$ (wedges $H_1^{(3)}$ and triangles $H_2^{(3)}$) given limited number of queries (or, equivalently, limited number of samples).
\item Variance and correlation of samples for a graphlet count estimator for 3-stars $H_1^{(4)}$ and 4-paths $H_2^{(4)}$.
\end{enumerate}

In all methods, the first vertex of the lifting procedure is taken from a standard random walk on vertices.
Whenever we specify the burn-in, that would correspond to the random walk steps taken to sample the starting vertex in case of Lifting and Waddling, and the number of steps taken between sampling CISs in case of PSRW.
We compare the Shotgun Lift estimator against Waddling estimator for the first problem, and compare the Unordered Lift estimator against PSRW and Waddling for the second problem.

\subsection{Relative Error given limited queries}

The brute force method gives us the exact graphlet count for $k=3$:

\begin{table}[th]
    \centering
    \begin{tabular}{|l|r|r|}
    \hline
    \multicolumn{3}{ |c| }{Graphlet counts for $k=3$}\\[2pt]
    \hline
    Network name & $H_1^{(3)}$ & $H_2^{(3)}$  \\[2pt]
    \hline
    \textbf{bio-celegansneural} & 4.408E+04 & 3.241E+03 \\[5pt]
    \textbf{ia-email-univ} & 8.038E+04 & 5.343E+03	\\[5pt]
    \textbf{misc-polblogs} & 1.038E+06 & 1.010E+05	\\[5pt]
    \textbf{misc-as-caida} & 1.479E+07 & 3.636E+04 \\[5pt]
    \textbf{misc-fullb} & 1.620E+08 & 6.021E+07 \\[2pt]
    \hline
    \end{tabular}
\end{table}

We will compare the relative error 
\begin{equation*}
    \text{relative error} = \frac{|\hat N_m(G) - N_m(G)|}{N_m(G)}
\end{equation*}
between Shotgun Lift estimator and Waddling estimator with burn-in $3$. The number of queries required for each sample $B\in V_G^{(2)}$ is $4$ for the Shotgun Lift algorithm and the number of queries for each $3$-CIS sample is $5$ for the Waddling algorithm.
We take the average error across $100$ runs.

As we see from the plots in Figure~\ref{fig:relative_errors}, Shotgun Lift method outperforms Waddling by a factor of 2 in most cases.
This agrees with our theory, since the number of actual CISs sampled by the Shotgun method is much bigger than Waddle given the same number of queries.

\subsection{Variation and correlation of samples}

We will use the Unordered Lift estimator, to count all motifs of size $4$, but the performance would be measured in terms of the variance of the estimator, both the "variance under independence" and "covariance" parts.
To get an idea about the distribution of $4$-graphlets, we count the $4$-graphlets with the Unordered Lift estimator.

\begin{table}[th]
    \centering
    \begin{tabular}{|l|r|r|r|}
    \hline
    \multicolumn{4}{ |c| }{Graphlet estimates for $k=4$}\\
    \hline
    Network name & $H_1^{(4)}$ & $H_2^{(4)}$ & $H_3^{(4)}$\\
    \hline
    \textbf{bio-celegansneural} &	6.48E+05 &	5.16E+05 &	1.86E+05 \\
    \textbf{ia-email-univ} &	5.40E+05 &	1.11E+06 & 	2.20E+05 \\
    \textbf{misc-polblogs} &	3.94E+07 & 	3.10E+07 & 	1.58E+07 \\
    \textbf{misc-as-caida} &	7.82E+09 &	2.85E+08 &	4.62E+07 \\
    \textbf{misc-fullb} &	1.07E+09 &	4.83E+09 &	2.71E+09 \\
    \hline
    \end{tabular}
\end{table}

%\begin{table}[th]
%    \centering
%    \begin{tabular}{|l|r|r|r|}
%    \hline
%    \multicolumn{4}{ |c| }{Graphlet estimates for $k=4$}\\
%    \hline
%    Network name & $H_4^{(4)}$ & $H_5^{(4)}$ & $H_6^{(4)}$\\
%    \hline
%    \textbf{bio-celegansneural} & 	1.66E+04 &	2.26E+04 & 	2.03E+03 %\\
%    \textbf{ia-email-univ} &	1.24E+04 &	2.04E+04 & 	3.46E+03 \\
%    \textbf{misc-polblogs} &	1.16E+06 &	2.82E+06 &	4.37E+05 \\
%    \textbf{misc-as-caida} &	4.08E+05 &	1.60E+06 &	4.44E+04 \\
%    \textbf{misc-fullb} &	6.43E+07 &	8.95E+08 &	3.71E+08 \\
%    \hline
%    \end{tabular}
%\end{table}

Denote $\phi_i = \frac{\ind(T_i\sim H_m)}{\pi(T_i)}$ for all estimators.
%Waddling, PSRW and Unordered Lift estimators.
Note that $\E \phi_1 = N_m(G)$.
We compare the variance of the estimator $\hat N_m$ using equation \eqref{eq:variance}.
The table below shows estimates for $V_m^{\independent}(\phi_1)$:

\begin{table}[th]
    \centering
    \begin{tabular}{ |l|c||r|r|r| }
    \hline
    \multicolumn{5}{ |c| }{Variation under independence} \\
    \hline
    Network Name & ID & Lift & Waddle & PSRW \\ 
    \hline
    \multirow{2}{*}{\textbf{bio-celegansneural}}
    % & $H_1^{(3)}$ & 0.740 &	1.716 &	0.219 \\
    % & $H_2^{(3)}$ & 5.061 &	6.264 &	4.553 \\
    & $H_1^{(4)}$ & 6.876 &	11.064 &	0.652 \\
    & $H_2^{(4)}$ & 5.470 &	3.956 &	5.185 \\
    \hline
    \multirow{2}{*}{\textbf{ia-email-univ}}
    % & $H_1^{(3)}$ & 0.465 &	0.916 &	0.197 \\
    % & $H_2^{(3)}$ & 6.193 &	6.885 &	5.052 \\
    & $H_1^{(4)}$ & 5.266 &	4.164 &	1.179 \\
    & $H_2^{(4)}$ & 3.272 &	2.958 &	2.216 \\
    \hline
    \multirow{2}{*}{\textbf{misc-polblogs}}
    % & $H_1^{(3)}$ & 0.614 &	1.452 &	0.287 \\
    % & $H_2^{(3)}$ & 4.368 &	4.951 &	3.483 \\
    & $H_1^{(4)}$ & 5.033 &	7.210 &	0.820 \\
    & $H_2^{(4)}$ & 6.214 &	5.369 &	6.093 \\
    \hline
    \multirow{2}{*}{\textbf{misc-as-caida}}
    % & $H_1^{(3)}$ & 1.895 &	4.731 &	0.006 \\
    % & $H_2^{(3)}$ & 176.0 &	246.1 &	165.3 \\
    & $H_1^{(4)}$ & 4.076 &	9.028 &	0.019 \\
    & $H_2^{(4)}$ & 106.06 &	153.11 &	78.87 \\
    \hline
    \multirow{2}{*}{\textbf{misc-fullb}}
    % & $H_1^{(3)}$ & 1.205 &	1.361 & 1.105\\
    % & $H_2^{(3)}$ & 0.911 & 0.957 & 0.904\\
    & $H_1^{(4)}$ & 21.617 &	10.185 &	6.179\\
    & $H_2^{(4)}$ & 6.014 &	4.041 & 3.785\\
    \hline
    \end{tabular}
\end{table}

We can see that PSRW generally has smaller variation under independence, mostly because of the probability $\pi(T)$ being independent of the degrees of vertices of $T$.
On the other hand, variation under independence of Lift and Waddle methods is comparable, meaning that $\pi(T)$ varies similarly for those two methods. 
Next, we compare the dependence of $\phi_i$ and $\phi_{i+1}$ using correlation for different values of the burn-in $h$ (see Fig.\ref{fig:correlation}).
For Lift and Waddling, the burn-in between $\phi_i$ and $\phi_{i+1}$ is the number of steps taken after sampling $T_i$ to get a new starting vertex for $T_{i+1}$.
For PSRW, burn-in is the number of steps between CIS samples in the random walk on subgraphs.
From the graphs in Figure~\ref{fig:correlation}, we see that PSRW produces highly correlated samples compared to Lift and Waddling methods.
This agrees with our analysis of PSRW, since it takes many more steps for the subgraph random walk to achieve desired mixing compared to the random walk on vertices.

\section{Supplement to "Estimating Graphlets via Lifting"}

\subsection{Proof of Prop. \ref{prop:unbiased}.}
    \begin{proof}
    Let $\phi_i$ be as defined in \eqref{phi:ord}, \eqref{phi:shot}.
    For both estimators, because of the form of \eqref{eq:ord_est} and \eqref{eq:shot_est}, if a single term $\phi_i$ is unbiased then $\hat N_m$ is as well.
    Let us begin with $\hat N_{O,m}$, by considering a draw from the lifting process, $A = [v_1,\ldots,v_k]$ which induces the $k$-subgraph, $G|A$.
    By the definition of $\tilde \pi$,
	\begin{equation*}
	    \E \left( \phi_{O,1} \right) = \sum_{A \in V_G^k} \tilde \pi(A) \left( \frac{\ind(T(A) \sim H_m)}{\co(T(A)) \tilde \pi(A)} \right) 
	    = \sum_{T \in \cV_k} \sum_{A \in V_G^k : T(A) = T} \frac{\ind(T \sim H_m)}{\co(T)} = \sum_{T \in \cV_k} \ind(T \sim H_m) = N_m.  
	\end{equation*}
	Hence, the $\hat N_{O,m}$ is unbiased.
	Consider the shotgun estimator, $\hat N_{S,m}$,
	\begin{multline*}
	    \E \left( \phi_{S,1} \right) = \sum_{B \in V_G^{k-1}} \tilde \pi(B) \sum_{u \in \cN_v(B)} \left( \frac{\ind(G|B \cup \{u\} \sim H_m)}{\co(H_m) \tilde \pi(B)} \right) \\
	    = \sum_{T \in \cV_k} \sum_{B \in V_G^{k-1}} \ind(G|B \cup \{u\} = T, u \in \cN_v(B)) \frac{\ind(T \sim H_m)}{\co(T)} 
	    = \sum_{T \in \cV_k} \ind(T \sim H_m) = N_m.  
	\end{multline*}
    Hence, the shotgun estimator is unbiased as well.
	\end{proof}


\subsection{Proof of Theorem \ref{thm:var_bd}.}
We can bound the variance in \eqref{eq:variation.ind} by the second moment, which is bounded by,
\begin{equation*}
    \E \phi_1^2 \leq \E \phi_1\max{\phi_1} = N_m(G)\max{\phi_1}.
\end{equation*}
Seeking to control the the maximum of $\phi_1$, we see that,
\begin{equation*}
    \max_T \frac{1}{\pi_U(T)} \le \max_A \frac{1}{|\co(T)| \tilde \pi(A)} \le 
    \max\frac{\prod_{r=1}^{k-1} (d_1+\ldots+d_r)}{|\co(H_m)|\pi_1(d_1)},
\end{equation*}
\begin{equation*}
    \max_B \frac{|\cN_v(B)|}{|\co(H_m)|\tilde\pi(B)} \le 
    \max\frac{\prod_{r=1}^{k-1} (d_1+\ldots+d_r)}{|\co(H_m)|\pi_1(d_1)}.
\end{equation*}
Thus, we can construct a bound on $V_m^{\independent}(\phi_1)$.

\subsection{Proof of Theorem \ref{thm:cor_bd}}

Let $\phi_i$ be as defined in \eqref{phi:ord}, \eqref{phi:shot} or \eqref{phi:unord}.
Given two starting vertices $v_i$ and $v_j$ of the lifting process, notice that random variables $\phi_i|v_i$ and $\phi_j|v_j$ are independent.
% Given large $n$, the covariance term from \eqref{eq:variation} can be approximated as follows
% \begin{multline*}
%     \frac{2}{n^2} \sum_{i < j} \Cov_{\pi_k}\left(\phi(T_i), \phi(T_j)\right) \approx
%     \frac{2}{n} \big( \Cov_{\pi_k}\left(\phi(T_t), \phi(T_{t+1})\right) \\
%     + \Cov_{\pi_k}\left(\phi(T_t), \phi(T_{t+2})\right) + \ldots\big).
% \end{multline*}
Therefore
\begin{equation*}
    \E\left(\phi_i \phi_{i+1})\right)=
    \E_{\pi_1(v_i)\times \pi_1(v_{i+1})} \E\left(\phi_i \phi_{i+1}| v_i, v_{i+1}\right) =\\
     \E_{\pi_1(v_i)\times \pi_1(v_{i+1})} \left(\E\left(\phi_i|v_i\right) \E\left(\phi_{i+1}|v_{i+1}\right)\right).
\end{equation*}

Using the equation above, we can bound the covariance of $\phi_i$ and $\phi_{i+1}$ with basic inequalities:
\begin{multline*}
    |\, \Cov\left(\phi_i, \phi_{i+1}\right)| \leq 
    \sum_{x_1,x_2\in V_G}  \E(\phi_i | \, v_i =  x_1)  \E\left(\phi_{i+1}|\, v_{i+1}=x_2\right)\ 
    \big\vert\,\P(v_i=x_1,v_{i+1}=x_2) - \pi_1(x_1)\pi_1(x_2)\,\big\vert\leq \\
    \max_{x_2\in V_G}  \E(\phi_{i+1}|\, v_{i+1} =x_2) \sum_{x_1} \E\left(\phi_i |\, v_i = x_1\right)
    \max_{x_1} \sum_{x_2}  \big\vert\,\P(v_i=x_1,v_{i+1}=x_2) - \pi(x_1)\pi(x_2)\,\big\vert = \\
      2\gamma_{G_V}(h) \max_{x_2} \,\E\left(\phi_{i+1}|\, v_{i+1}=x_2\right) \sum_{x_1} \E\left(\phi_i|\, v_i = x_1\right),
\end{multline*}

where $\gamma_{G_V}(h)$ is the mixing coefficient from \eqref{eq:gamma} for the random walk on vertices.
Next, estimate factors from the RHS as follows:
\begin{equation}
    \sum_{x}\E\left(\phi|v = x\right) \leq
    \max_{x}\frac{1}{\pi(x)} \sum_{x}\E\left(\phi|v = x\right) \pi(x)\leq
    2|E_G| N_m(G).
\end{equation}

For $\max_x \E\left(\phi|v=x\right)$, consider the expressions for $\phi$ from \eqref{phi:ord}, \eqref{phi:shot} or \eqref{phi:unord}.

Using notation $D = \prod_{r=2}^{k-1} (\Delta_1 +\ldots + \Delta_r)$, for the Ordered Lift estimator,
\begin{equation*}
    \max_x \E\left(\phi_O|v=x\right) \le \max_x \sum_{A} \frac{\P(A|v=x)}{\tilde\pi(A)} \le
    \max_x \frac{\# \{ A\mid A[1]=x \}}{\pi(x)} \le 2|E_G|D.
\end{equation*}
For the Shotgun Lift estimator,
\begin{equation*}
    \max_x \E\left(\phi_S|v=x\right) \le
    \max_x \sum_B |\cN_v(B)|\frac{\P(B|v=x)}{\tilde\pi(B)} \leq 
    \max_x \frac{|\cN_v(B)|\, \#\{ B\mid B[1]=x \}}{\pi(x)} \leq
    2|E_G| D,
\end{equation*}
For the Unordered Lift estimator,
\begin{equation*}
    \max_x \E\left(\phi_U|v=x\right) \le
    \max_x \sum_T \frac{\P(T|v=x)}{\pi_U(T)} \leq 
    \max_x \frac{\#\{T\mid x\in V_T\}}{\pi(x)} \leq
    2|E_G| D.
\end{equation*}

Combining the results, we get the desired bound.


\section{Conclusion}
We introduced a flexible methodology for sampling graphlets, called Lifting.
Our experimental results demonstrate our hypothesis that lifting can have smaller variance than Waddling, and better mixing rates than Subgraph Random Walking.
This was reinforced by our theoretical results that control the variance of and covariance between graphlet updates.
We anticipate that this lifting scheme will have more far reaching implications, as we may apply graphlet sampling to supervised learning over graphs and other machine learning applications. 

~\newline
%\section{Conclusion}
\noindent
{\bf Acknowledgements:} JS is supported by NSF DMS-1712996.
%\subsection*{Acknowledgments}

%Grant support, people that helped.


% Then
% \begin{multline*}
%     \frac{2}{n^2} \left\vert\sum_{i < j} \Cov_{\pi_k}\left(\phi(T_i), \phi(T_j)\right) \right\vert \leq\\
%     \frac{16}{n} N_m(G)|E_G|^2 D \sum_{h=1}^\infty \gamma_X(h\tau) \leq \\
%     \frac{16}{n} N_m(G)|E_G|^2 D \frac{e^{-(1-\mu)\tau}}{1-e^{-(1-\mu)\tau}}.
% \end{multline*}

% \subsection{Mixing Time}
% In order to assess the mixing time of the we will use the concept of $\beta$-mixing.
% \begin{definition}
%   The $\beta$-mixing coefficient of a Markov chain with discrete state space, $X_t \in \mathcal X$, is
%   \begin{multline*}
%     \beta_X(t,h) :=\\
%     \sum_{x_0,x_1 \in \mathcal X} |\mathbb P\{ X_t = x_0, X_{t+h} = x_1\} - \mathbb P\{ X_t = x_0 \} \mathbb P\{ X_{t+h} = x_1 \} |.
%   \end{multline*}
% \end{definition}

% \begin{theorem}
%   Let $X_t$ be the random walk over the graph $G$ and let $Y_{t}$ be the graphlet extracted from the $k$-lifting process.
%   Then
%   \begin{equation}
%     \mathbb \beta_X(t,h) \ge \mathbb \beta_Y(t,h).
%   \end{equation}
% \end{theorem}

% \begin{proof}
%   Let $Q_1 ( x_t, x_{t+h} ) = \mathbb P \{ X_t = x_t, X_{t+h} = x_{t+h} \}$ and $Q_2 = \mathbb P \{ X_t = x_t\} \mathbb P\{X_{t+h} = x_{t+h}\}$.
%   Also, let $P_1, P_2$ be the joint distributions of $Y_t, Y_{t+h}$ when $X_t, X_{t+h}$ are drawn according to $Q_1,Q_2$ respectively.
%   Due to the lifting process, the distribution of $Y_t, Y_{t+h} | X_t, X_{t+h}$ is invariant to the choice of distribution of $X_t, X_{t+h}$ (being from $Q_1$ or $Q_2$).
%   Hence, by the generalized data processing inequality we have that
%   \begin{equation*}
%     D_f(Q_1 || Q_2) \ge D_f(P_1 || P_2),
%   \end{equation*}
%   for any $f$-divergence.
%   By selecting the total variation divergence, we have the result.
% \end{proof}

